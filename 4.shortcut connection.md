Have you heard about a social experiement. There are many peoples in a line, and you give a message to the people in the head of the line. Then you ask people in the line to pass the message from the
first one to the next one. When the message come to the last one, you goto check the message comming out from the last people, you will find the final content of the final message is already mutated 
to a totally new one and it has nothing to do with the original message passed to the first one.

When the message is passing to each one, each guy receiving the message will lost and misunstood some part of the content of the message and make some remedy some part of it, that's why when the message
comes out from the last guy, it turns into a totally new one. That's the same for deep learning network. As we have seen that, the network is composed by many layers which just like people in a line.Data
passed into the first layer, and the layer do some computation on its input and make its output, this is just like people receiving the message, losing some original content and make up some new info
into the original one.

This characteristics will cause some problem to the network. Because the orignial content may mutated when passing through layers and the final output may unsatisfy since the lossing content of each
layer. One way to ensure that the original message can be received by the final people is we send the original message to the next people when the message has passed through some number of people. 
This can apply to deep learning network. When the original input send to the first layer, then we send the same input to the tenth layer and send the original message to the twentieth layer and so on.
This trick to help the network remember the original input data is what we call shortcut connection.

In implementation, the shortcut connection is quit simple, we just add a the output of given output to another layer deeper than it. It may easier to understand this by looking at the code:
```py
class ShortCutConnectionExampleNetwork(nn.Module):
  def __init__(self, layer_sizes, use_shortcut):
    super().__init__()
    self.use_shortcut = use_shortcut
    #arrange layers into a line
    self.layers = nn.ModuleList([
        nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),
        nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),
        nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),
        nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),
        nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU()),
    ])

  def forward(self, x):
    for layer in self.layers:
      #passing message from first guy to next one until the last one
      layer_output = layer(x) 
      if self.use_shortcut and x.shape == layer_output.shape:
        #apply shortcut here, just like tell the given guy in the line about the original message
        x = x + layer_output
      else:
        x = layer_output
    return x

layer_sizes = [3, 3, 3, 3, 3, 3, 1]
original_input = torch.tensor([[1., 0. , -1.]])
torch.manual_seed(246)
#we need to compare about the differences about using or not using shortcut
model_without_shortcut = ShortCutConnectionExampleNetwork(layer_sizes, use_shortcut = False)
model_with_shortcut = ShortCutConnectionExampleNetwork(layer_sizes, use_shortcut = True)

def print_gradients(model, x):
  output = model(x)
  target = torch.tensor([[3.14]])
  loss = nn.MSELoss()
  loss = loss(output, target)
  #compute gradients use to guid the network how to achive the given target
  loss.backward()
  for name, param in model.named_parameters():
    if 'weight' in name:
      print(f"{name} has gradient mean of {param.grad.abs().mean().item()}")

print("gradients for network without shortcut:")
print_gradients(model_without_shortcut, original_input)
print("gradients for network with shortcut:")
print_gradients(model_with_shortcut, original_input)
```
Running above code we get the following result:
```py
gradients for network without shortcut:
layers.0.0.weight has gradient mean of 0.002455572597682476
layers.1.0.weight has gradient mean of 0.0017375850584357977
layers.2.0.weight has gradient mean of 0.006028742529451847
layers.3.0.weight has gradient mean of 0.053520843386650085
layers.4.0.weight has gradient mean of 0.1556416004896164
gradients for network with shortcut:
layers.0.0.weight has gradient mean of 0.8956885933876038
layers.1.0.weight has gradient mean of 0.44546276330947876
layers.2.0.weight has gradient mean of 0.5507633686065674
layers.3.0.weight has gradient mean of 0.38325634598731995
layers.4.0.weight has gradient mean of 0.38575783371925354
```
compare the gradients for network with and without shortcut, the gradients for network with shortcut is greater than network without shortcut. That means the adjustment for network without shortcut
is too small to guid the network to achive the given target, that is similar to the vanishing gradient problem we seen before. Network without shortcut will have more trainning time and converge 
much more slowly than network with shortcut.

