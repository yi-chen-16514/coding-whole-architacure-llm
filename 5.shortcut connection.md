Have you heard about a social experiement. There are many peoples in a line, and you give a message to the people in the head of the line. Then you ask people in the line to pass the message from the
first one to the next one. When the message come to the last one, you goto check the message comming out from the last people, you will find the final content of the final message is already mutated 
to a totally new one and it has nothing to do with the original message passed to the first one.

When the message is passing to each one, each guy receiving the message will lost and misunstood some part of the content of the message and make some remedy some part of it, that's why when the message
comes out from the last guy, it turns into a totally new one. That's the same for deep learning network. As we have seen that, the network is composed by many layers which just like people in a line.Data
passed into the first layer, and the layer do some computation on its input and make its output, this is just like people receiving the message, losing some original content and make up some new info
into the original one.

This characteristics will cause some problem to the network. Because the orignial content may mutated when passing through layers and the final output may unsatisfy since the lossing content of each
layer. One way to ensure that the original message can be received by the final people is we send the original message to the next people when the message has passed through some number of people. 
This can apply to deep learning network. When the original input send to the first layer, then we send the same input to the tenth layer and send the original message to the twentieth layer and so on.
This trick to help the network remember the original input data is what we call shortcut connection.
