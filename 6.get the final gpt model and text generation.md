In this last section, we will see how to assemble the final gpt model from all components we created before. Actually we have been passing the most difficult parts of GPT that is 
the attention mechanism. Now what we need to do is to assembly all those components together to get the final model as following:

<img width="545" height="766" alt="空白流程图" src="https://github.com/user-attachments/assets/f34832e5-e955-465e-93f4-52858e8bd8b2" />

As we can see from above image, the GPT model is actually repeat the transformer block several times the we will done. Now Let's check the code for doing above steps:

```py
class GPTModel(nn.Module):
  def __init__(self, cfg):
    super().__init__()
    self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
    self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
    self.drop_emb = nn.Dropout(cfg["drop_rate"])

    self.trf_blocks = nn.Sequential(
        *[TransformerBlock(cfg) for _ in range(cfg["n_layers"])]
    )

    self.final_norm = LayerNormalization(cfg["emb_dim"])
    self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias = False)

  def forward(self, in_idx):
    batch_size, seq_len = in_idx.shape
    tok_embeds = self.tok_emb(in_idx)

    pos_embeds = self.pos_emb(torch.arange(seq_len, device = in_idx.device))
    x = tok_embeds + pos_embeds
    x = self.drop_emb(x)
    x = self.trf_blocks(x)
    x = self.final_norm(x)
    logits = self.out_head(x)
    return logits
```
Then we can try to run above code as following:

```py
torch.manual_seed(123)
model = GPTModel(GPT_MODEL_CONFIG)

out = model(batch)
print("Input batch: \n", batch)
print(f"\nOutput shape: {out.shape}")
print(out)
```

Then we can get the following result:
```py
nput batch: 
 tensor([[29688,   423,   257,   922],
        [  548,  3621,   284,   766]])

Output shape: torch.Size([2, 4, 50257])
tensor([[[-1.0453,  0.1800, -0.0803,  ..., -1.0559, -0.9745, -0.3132],
         [-0.7219, -0.2006, -0.2538,  ..., -0.6076, -0.9438, -0.3187],
         [-0.7586,  0.1957, -0.5045,  ..., -0.1706, -0.2094, -0.1008],
         [ 0.1775, -0.2065,  0.6359,  ...,  0.4498,  0.6831,  0.5175]],

        [[-1.0495, -0.5007, -0.7872,  ..., -0.6699, -0.6705, -0.1962],
         [-0.4351, -0.5589, -0.8543,  ..., -0.5571, -0.5653,  0.3484],
         [-0.4010, -0.3221,  0.3346,  ..., -0.1267,  0.2264,  0.4347],
         [-0.4100, -1.1526,  1.4263,  ...,  0.7129, -0.1073,  1.0928]]],
```
To be noticed, the finall result is vector with elements of the vocab size, and the last vector contains probability of each word of the vocab for the next output word.

Finally let's see how to use the model to predict next word or generate setence given by the input, for example we have 4 words currently: w1, w2, w3, w4, then we want to
predict the next 3 words, then we will do as following:
model([tok_idx_w1, tok_idx_w2, tok_idx_w3, tok_idx_w4]) => [tok_idx_w1, tok_idx_w2, tok_idx_w3, tok_idx_w4, tok_idx_w5]
model([tok_idx_w1, tok_idx_w2, tok_idx_w3, tok_idx_w4, tok_idx_w5]) => [tok_idx_w1, tok_idx_w2, tok_idx_w3, tok_idx_w4, tok_idx_w5, tok_idx_w6]
model([tok_idx_w1, tok_idx_w2, tok_idx_w3, tok_idx_w4, tok_idx_w5, tok_idx_w6]) => [tok_idx_w1, tok_idx_w2, tok_idx_w3, tok_idx_w4, tok_idx_w5, tok_idx_w6, tok_idx_w7],

let's check the code for doing above steps:

```py
def generate_text_simple(model, idx, max_new_tokens, context_size):
  print(f"idx: {idx}, shape: {idx.shape}")
  for _ in range(max_new_tokens):
    '''
    idx = [[1,2,3,4], [1,2,3,4,5], [1,2,3,4,5,6]]-> idx_cond= [1,2,3,4,5,6]
    '''
    idx_cond = idx[:, -context_size:]
    print(f"idx_cond: {idx_cond}")
    with torch.no_grad():
      logits = model(idx_cond)
    
    '''
    input => ["I", "like", "to"]
    logits = [
    #base on "I" to predict like, the index of the biggest element would be the word index of 'like' in the vocab
    [[0.1, 0.2, 0.7], ...], 
    
    # base on "I like" to predict to
    [[0.3, 0.6, 0.1], ...],  # predict "to"
    
    # base on "I like to" to  predict eat
    [[0.8, 0.1, 0.1], ...],  # 
]
    '''
    print(f"original logits: {logits}")
    logits = logits[:, -1, :]
    print(f"processed logits: {logits}")
    probas = torch.softmax(logits, dim=-1)
    idx_next = torch.argmax(probas, dim=-1, keepdim=True)
    print(f"predicted next word idx: {idx_next}")
    idx = torch.cat((idx, idx_next), dim = 1)

  return idx

model.eval() #this indicate model is not in traning state then dropout is disabled
out = generate_text_simple(model = model, idx = encoded_tensor, max_new_tokens = 6, context_size = GPT_MODEL_CONFIG["context_length"])
print(f"Output: {out}")
print(f"Output length: {len(out[0])}")
```
The result of running above code should like:
```py
idx: tensor([[15496,    11,   314,   716]]), shape: torch.Size([1, 4])
idx_cond: tensor([[15496,    11,   314,   716]])
original logits: tensor([[[-1.2153, -0.6077, -0.6288,  ..., -1.0762, -0.5202, -0.5768],
         [-0.5942, -0.3256, -0.5996,  ..., -0.7276, -0.4339, -0.5254],
         [ 0.3721,  0.2682, -0.2465,  ..., -0.8220,  0.0132, -0.0590],
         [-0.3838, -0.7425,  0.3390,  ..., -0.0450,  0.7011, -0.2643]]])
processed logits: tensor([[-0.3838, -0.7425,  0.3390,  ..., -0.0450,  0.7011, -0.2643]])
predicted next word idx: tensor([[12192]])
idx_cond: tensor([[15496,    11,   314,   716, 12192]])
original logits: tensor([[[-1.2153, -0.6077, -0.6288,  ..., -1.0762, -0.5202, -0.5768],
         [-0.5942, -0.3256, -0.5996,  ..., -0.7276, -0.4339, -0.5254],
         [ 0.3721,  0.2682, -0.2465,  ..., -0.8220,  0.0132, -0.0590],
         [-0.3838, -0.7425,  0.3390,  ..., -0.0450,  0.7011, -0.2643],
         [ 0.7136, -0.7193,  0.3321,  ..., -0.1434, -0.6499,  1.1475]]])
processed logits: tensor([[ 0.7136, -0.7193,  0.3321,  ..., -0.1434, -0.6499,  1.1475]])
predicted next word idx: tensor([[17592]])
idx_cond: tensor([[15496,    11,   314,   716, 12192, 17592]])
original logits: tensor([[[-1.2153, -0.6077, -0.6288,  ..., -1.0762, -0.5202, -0.5768],
         [-0.5942, -0.3256, -0.5996,  ..., -0.7276, -0.4339, -0.5254],
         [ 0.3721,  0.2682, -0.2465,  ..., -0.8220,  0.0132, -0.0590],
         [-0.3838, -0.7425,  0.3390,  ..., -0.0450,  0.7011, -0.2643],
         [ 0.7136, -0.7193,  0.3321,  ..., -0.1434, -0.6499,  1.1475],
         [-0.2831, -0.1373, -0.0682,  ..., -0.0702,  0.3414, -0.4769]]])
processed logits: tensor([[-0.2831, -0.1373, -0.0682,  ..., -0.0702,  0.3414, -0.4769]])
```
Let's have more insight of the result:
```py
processed logits: tensor([[-0.3838, -0.7425,  0.3390,  ..., -0.0450,  0.7011, -0.2643]])
predicted next word idx: tensor([[12192]])
```

The processed logits is the vector of the probability of each word as the next word for the sentence, and the predicted next word idx is the index of the element with the 
biggest value in the vector, and this index would be used to identify the word string from the vocab.

