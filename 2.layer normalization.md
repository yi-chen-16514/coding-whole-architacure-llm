Let's fill in the first whole in the skeleton. In this section we will going to see how to implement layer normalization.Think about this, you are in point A, and you want to goto point B.You can only reach B by walking and at each step you have two choices turn left or turn right. In order to get it right, when taking the step you ask two guys for direction, if the first guy say "turn right for 200 kilomiters", and the second guy say "turn left for 100 kilometers", you need to combine two advices into a final result, then you take "turn right 200 kilometers" as positive 200, and "turn left for 100 kilomiters" for minus 100, then the final decision should be turn right 100 kilometers.

The problem here is , it is very time and energy comsuming for walking 100 kilometers, and if it is wrong, you need to "undo" the 100 kilometers' walking. That is the changing value is too huge and taking the change would cause too much.

Think another case, if the first guy say "turn right 2 millimeter“， and the second guy say "turn left 1 millimeter", then the final decision is to take left with 1 millimeter. This is also infeasible 
since you can have a step as small as 1 millimeter.

The best case is , the difference for advices from two guys is in the range of (0.5 miter， 1 miter), which means at each step, you may take 1 step for turning right or 1 step for turning left. The case for value too huge is what we say gradient exloding, and the case for value too small is what we say gradient vanishing". In order to make sure the changing value alway in the reasonable range, we need to take the trick of layer normalization, first let's use code to see the process for layer normalization:

```py

```
