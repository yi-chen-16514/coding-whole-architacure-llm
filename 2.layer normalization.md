Let's fill in the first whole in the skeleton. In this section we will going to see how to implement layer normalization.Think about this, you are in point A, and you want to goto point B.You can only reach B by walking and at each step you have two choices turn left or turn right. In order to get it right, when taking the step you ask two guys for direction, if the first guy say "turn right for 200 kilomiters", and the second guy say "turn left for 100 kilometers", you need to combine two advices into a final result, then you take "turn right 200 kilometers" as positive 200, and "turn left for 100 kilomiters" for minus 100, then the final decision should be turn right 100 kilometers.

The problem here is , it is very time and energy comsuming for walking 100 kilometers, and if it is wrong, you need to "undo" the 100 kilometers' walking. That is the changing value is too huge and taking the change would cause too much.

Think another case, if the first guy say "turn right 2 millimeter“， and the second guy say "turn left 1 millimeter", then the final decision is to take left with 1 millimeter. This is also infeasible 
since you can have a step as small as 1 millimeter.

The best case is , the difference for advices from two guys is in the range of (0.5 miter， 1 miter), which means at each step, you may take 1 step for turning right or 1 step for turning left. The case for value too huge is what we say gradient exloding, and the case for value too small is what we say gradient vanishing". In order to make sure the changing value alway in the reasonable range, we need to take the trick of layer normalization, first let's use code to see the process for layer normalization:

```py
import numpy as np

def layer_normalization(x, gamma=1.0, beta=0.0, eps=1e-5):
    """
    Simulate Layer Normalization algorithm steps using 1D array
    
    Parameters:
    x: 1D array representing layer outputs in a neural network
    gamma: scaling parameter (learnable)
    beta: shifting parameter (learnable) 
    eps: small constant to prevent division by zero
    
    Returns:
    y: normalized output
    """
    
    print("Layer Normalization Algorithm Steps:")
    print("=" * 50)
    
    # Step 1: Calculate mean
    mean = np.mean(x)
    print(f"Step 1 - Calculate Mean:")
    print(f"  Input: {x}")
    print(f"  Mean (μ) = {mean:.6f}")
    print(f"  Formula: μ = (1/{len(x)}) * Σx_i")
    
    # Step 2: Calculate variance
    variance = np.var(x)
    print(f"\nStep 2 - Calculate Variance:")
    print(f"  Variance (σ²) = {variance:.6f}")
    print(f"  Formula: σ² = (1/{len(x)}) * Σ(x_i - μ)²")
    
    # Step 3: Calculate standard deviation
    std = np.sqrt(variance + eps)
    print(f"\nStep 3 - Calculate Standard Deviation:")
    print(f"  Standard Deviation (σ) = √(σ² + ε) = {std:.6f}")
    
    # Step 4: Normalize (subtract mean, divide by std)
    x_normalized = (x - mean) / std
    print(f"\nStep 4 - Normalize:")
    print(f"  Normalized: (x - μ) / σ = {x_normalized}")
    
    # Step 5: Scale and shift with learnable parameters
    y = gamma * x_normalized + beta
    print(f"\nStep 5 - Scale and Shift:")
    print(f"  Scaling: γ * normalized = {gamma} * {x_normalized}")
    print(f"  Shifting: + β = + {beta}")
    print(f"  Final Output: {y}")
    
    return y

# Test with a specific 1D array
if __name__ == "__main__":
    # Example 1D array simulating neural network layer outputs
    test_array = np.array([2.0, 4.0, 6.0, 8.0, 10.0])
    
    print("Testing Layer Normalization with 1D array:")
    print("Input array (simulating layer outputs):", test_array)
    print()
    
    # Apply layer normalization
    normalized_output = layer_normalization(test_array)
    
    print("\n" + "=" * 50)
    print("Summary:")
    print(f"Input: {test_array}")
    print(f"Normalized Output: {normalized_output}")
    print(f"Input Statistics - Mean: {np.mean(test_array):.4f}, Std: {np.std(test_array):.4f}")
    print(f"Output Statistics - Mean: {np.mean(normalized_output):.4f}, Std: {np.std(normalized_output):.4f}")
```
Running above code we get the following result:
```py
Testing Layer Normalization with 1D array:
Input array (simulating layer outputs): [ 2.  4.  6.  8. 10.]

Layer Normalization Algorithm Steps:
==================================================
Step 1 - Calculate Mean:
  Input: [ 2.  4.  6.  8. 10.]
  Mean (μ) = 6.000000
  Formula: μ = (1/5) * Σx_i

Step 2 - Calculate Variance:
  Variance (σ²) = 8.000000
  Formula: σ² = (1/5) * Σ(x_i - μ)²

Step 3 - Calculate Standard Deviation:
  Standard Deviation (σ) = √(σ² + ε) = 2.828429

Step 4 - Normalize:
  Normalized: (x - μ) / σ = [-1.41421268 -0.70710634  0.          0.70710634  1.41421268]

Step 5 - Scale and Shift:
  Scaling: γ * normalized = 1.0 * [-1.41421268 -0.70710634  0.          0.70710634  1.41421268]
  Shifting: + β = + 0.0
  Final Output: [-1.41421268 -0.70710634  0.          0.70710634  1.41421268]

==================================================
Summary:
Input: [ 2.  4.  6.  8. 10.]
Normalized Output: [-1.41421268 -0.70710634  0.          0.70710634  1.41421268]
Input Statistics - Mean: 6.0000, Std: 2.8284
Output Statistics - Mean: -0.0000, Std: 1.0000
```
As we can see from the code, layer normalization is putting some statistic operation on the output of given layer, the operation result would be the input for the next layer. We don't need to do 
these by ourself, we can rely on pytorch to do the job such as the code here:
```py
import torch
import torch.nn as nn
class LayerNormalization(nn.Module):
  def __init__(self, emb_dim):
    super().__init__()
    #emb_dim is the length of input vector
    self.eps = 1e-5
    self.gamma = nn.Parameter(torch.ones(emb_dim))
    self.beta = nn.Parameter(torch.zeros(emb_dim))

  def forward(self, x):
    mean = x.mean(dim = -1, keepdim = True)
    var = x.var(dim = -1, keepdim = True, unbiased = False)
    norm_x = (x - mean) / torch.sqrt(var + self.eps)
    return self.gamma * norm_x + self.beta
```
Now let's try to run above code:
```py
torch.manual_seed(456)
example = torch.randn(1,5)
ln = LayerNormalization(emb_dim=5)
out_ln = ln(example)
print(f"normalized output: {out_ln}")
mean = out_ln.mean(dim=-1, keepdim=True)
var = out_ln.var(dim=-1, unbiased=False, keepdim=True)
print(f"Mean: {mean}")
print(f"Variance: {var}")
```
The ourput for above code is:
```py
normalized output: tensor([[-1.5236,  0.7289,  1.3287, -0.6130,  0.0790]], grad_fn=<AddBackward0>)
Mean: tensor([[2.3842e-08]], grad_fn=<MeanBackward1>)
Variance: tensor([[1.0000]], grad_fn=<VarBackward0>)
```
As we can see from the output, the mean is actually 0 and the variance is 1 which is exactly what we want


