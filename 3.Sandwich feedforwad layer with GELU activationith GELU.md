In this section, we fill in the third hole with a sandwich like layer structure as following:

<img width="858" height="1238" alt="whiteboard_exported_image (2)" src="https://github.com/user-attachments/assets/baf62a9c-e3a9-4764-b79b-61adf4803aa0" />

From above image, we can see actions taken by this layer, first it will using a linear mapping to transfer the input vector from lower dimension to higher dimension, for eample , it wii receive
a vector with 768 elemnts to a new vector with elemts of 3072 elemnts, such transfer is simple, we can multiply the vector with a matrix with rows of 768 and columns of 3072, after getting the
newly enlarged vector, we process the vector with a function name GELU, its mathmatic format is as following:



![screenshot_20251018_151306](https://github.com/user-attachments/assets/73871a1a-bfaa-4477-9a3f-53b2c768675c)

Let's using code to explore the GELU function and check its characteristic:
```py
import torch
import matplotlib.pyplot as plt
import torch.nn as nn


class GELU(nn.Module):
  def __init__(self):
    super().__init__()

  def forward(self, x):
    return 0.5 * x * (1 + torch.tanh(torch.sqrt(torch.tensor(2.0 / torch.pi)) * (x + 0.044715 * torch.pow(x, 3))))

gelu, relu = GELU(), nn.ReLU()
x = torch.linspace(-3, 3, 1000)
y_gelu, y_relu = gelu(x), relu(x)
plt.figure(figsize=(8,3))
for i, (y, label) in enumerate(zip([y_gelu, y_relu], ["GELU", "RELU"]), 1):
  plt.subplot(1, 2, i)
  plt.plot(x, y)
  plt.title(f"{label} activation function")
  plt.xlabel("x")
  plt.ylabel(f"{label}(x)")
  plt.grid(True)

plt.tight_layout()
plt.show()
```
Running above code we get the following result:


<img width="790" height="290" alt="image" src="https://github.com/user-attachments/assets/9eae6dbd-2cd1-4e48-bbd7-c39563c8b247" />

Checking above image, we can compare the advantage of gelu , it can compute derivate in any where and the derivates in each point of gelu is not 0, which means the whole model can find its refineing
directions in each point, but for relu, the derivates for points left of 0 are all 0, which means the whole model will stand still for any negative points, for any huge model like gpt, the training
speed will greately slower down if you have lots of points which there derivates are 0.

Now let's complete the whole layer indicated by the first image by following code:

```py
class FeedForward(nn.Module):
  def __init__(self, cfg):
    super().__init__()
    self.layers = nn.Sequential(
        nn.Linear(cfg["emb_dim"], 4 * cfg["emb_dim"]),
        GELU(),
        nn.Linear(4 * cfg["emb_dim"], cfg["emb_dim"])
    )

  def forward(self, x):
    return self.layers(x)

ffn = FeedForward(GPT_MODEL_CONFIG)
x = torch.rand(2, 3, 768)
out = ffn(x)
print(f"output result:{out}\n output shape:{out.shape}") 
```
And running above code we get result like following:
```py
output result:tensor([[[-0.0284, -0.1283, -0.0517,  ..., -0.0856,  0.0744,  0.0662],
         [-0.0316, -0.2602, -0.0380,  ..., -0.0508,  0.1694,  0.0745],
         [-0.0026, -0.0782,  0.0732,  ...,  0.0887,  0.1524,  0.0813]],

        [[-0.1388, -0.1519, -0.0214,  ..., -0.0036,  0.1533,  0.0177],
         [ 0.0096, -0.1428, -0.0226,  ..., -0.0303,  0.1674, -0.0466],
         [-0.0498, -0.0990,  0.0242,  ..., -0.0756,  0.0401,  0.0162]]],
       grad_fn=<ViewBackward0>)
 output shape:torch.Size([2, 3, 768])
```

