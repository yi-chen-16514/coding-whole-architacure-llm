In this section, we will combine all tricks we learned before and connect them with multi-head attention block to create the most important component 
for LLM whic is the transformer block. Let's check its structure first:


<img width="741" height="1308" alt="transformer block" src="https://github.com/user-attachments/assets/14f43203-4018-4310-ab04-3fe904313199" />

The above graph depicts the transformer block, as we have seen and done with each element in the block, we combined all elements we done before to create the main part of the
transformer network. First the input vector is go through the layer normalization block which we make all elements for the input vector to sum up as 1.0. The the result go 
through masked multi-head attetion, the the output go through dropout block which randomly clear some elements from the output of masked multi-head to 0. Pay attentio here,
we need a shourt cut to send the original input vector to add with the output of the dropout block, this is used to prevent information missing after the input vector has
gone throung those blocks.

The the output of the short cut go through a layer normalization again which make all elements sums up to 1.0. The the output go through the feed fowrd layer which is is to
transform the input by using the GELU activation, then the output go through dropout again which randomly clear some elements to 0, and the final output is created by adding
up the output from the first dropout layer with the final output


